# -*- coding: utf-8 -*-
"""Real Estate Price Predictor

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-DaSwyTIVpHrugM4-3iD72lN-5X1nEi
"""

# real_estate_price_predictor.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import joblib
import requests
import folium
from geopy.geocoders import Nominatim
from geopy.distance import geodesic
import warnings
warnings.filterwarnings('ignore')

class RealEstatePricePredictor:
    def __init__(self):
        self.model = None
        self.preprocessor = None
        self.feature_names = []
        self.scaler = StandardScaler()
        self.is_trained = False

    def load_data(self, data_source='boston', custom_path=None):
        """Load real estate data from various sources"""
        if data_source == 'boston':
            # Load Boston Housing dataset
            from sklearn.datasets import load_boston
            boston = load_boston()

            df = pd.DataFrame(boston.data, columns=boston.feature_names)
            df['PRICE'] = boston.target

            # Add more realistic column names
            column_mapping = {
                'CRIM': 'crime_rate',
                'ZN': 'residential_zones',
                'INDUS': 'industrial_area',
                'CHAS': 'charles_river',
                'NOX': 'nitric_oxide',
                'RM': 'avg_rooms',
                'AGE': 'age_of_homes',
                'DIS': 'employment_distance',
                'RAD': 'highway_accessibility',
                'TAX': 'property_tax',
                'PTRATIO': 'pupil_teacher_ratio',
                'B': 'black_population',
                'LSTAT': 'lower_status_population',
                'PRICE': 'price'
            }
            df = df.rename(columns=column_mapping)

        elif data_source == 'california':
            # Load California Housing dataset
            from sklearn.datasets import fetch_california_housing
            california = fetch_california_housing()

            df = pd.DataFrame(california.data, columns=california.feature_names)
            df['price'] = california.target

        elif data_source == 'custom' and custom_path:
            # Load custom dataset
            df = pd.read_csv(custom_path)

        else:
            # Generate synthetic realistic data
            df = self.generate_synthetic_data()

        print(f"Loaded dataset with shape: {df.shape}")
        print(f"Columns: {list(df.columns)}")

        return df

    def generate_synthetic_data(self, n_samples=10000):
        """Generate synthetic real estate data"""
        np.random.seed(42)

        # Base features
        data = {
            'bedrooms': np.random.randint(1, 6, n_samples),
            'bathrooms': np.random.randint(1, 4, n_samples),
            'square_feet': np.random.normal(2000, 800, n_samples),
            'lot_size': np.random.normal(8000, 3000, n_samples),
            'year_built': np.random.randint(1950, 2024, n_samples),
            'garage_spaces': np.random.randint(0, 4, n_samples),
            'floors': np.random.randint(1, 4, n_samples),
        }

        # Location-based features
        locations = ['Downtown', 'Suburb', 'Rural', 'Waterfront', 'Mountain']
        data['location'] = np.random.choice(locations, n_samples)

        # Property type
        property_types = ['Single Family', 'Condo', 'Townhouse', 'Multi-Family']
        data['property_type'] = np.random.choice(property_types, n_samples)

        # Condition and quality
        data['condition'] = np.random.randint(1, 6, n_samples)  # 1-5 scale
        data['quality'] = np.random.randint(1, 6, n_samples)   # 1-5 scale

        # Distance to amenities (km)
        data['distance_to_city'] = np.random.exponential(15, n_samples)
        data['distance_to_school'] = np.random.exponential(3, n_samples)
        data['distance_to_shopping'] = np.random.exponential(5, n_samples)

        # Market features
        data['crime_rate'] = np.random.exponential(2, n_samples)
        data['school_rating'] = np.random.normal(7, 2, n_samples)

        df = pd.DataFrame(data)

        # Ensure positive values where appropriate
        df['square_feet'] = np.abs(df['square_feet'])
        df['lot_size'] = np.abs(df['lot_size'])
        df['school_rating'] = np.clip(df['school_rating'], 1, 10)

        # Calculate price based on features (realistic formula)
        base_price = 100000
        price = (
            base_price +
            df['square_feet'] * 120 +
            df['bedrooms'] * 15000 +
            df['bathrooms'] * 8000 +
            df['lot_size'] * 5 +
            (2024 - df['year_built']) * -500 +
            df['garage_spaces'] * 5000 +
            df['condition'] * 10000 +
            df['quality'] * 15000 +
            (10 - df['school_rating']) * -5000 +
            df['crime_rate'] * -3000
        )

        # Location multipliers
        location_multipliers = {
            'Downtown': 1.4, 'Waterfront': 1.6, 'Mountain': 1.2,
            'Suburb': 1.1, 'Rural': 0.8
        }

        for location, multiplier in location_multipliers.items():
            mask = df['location'] == location
            price[mask] *= multiplier

        # Property type multipliers
        type_multipliers = {
            'Single Family': 1.0, 'Condo': 0.8,
            'Townhouse': 0.9, 'Multi-Family': 1.3
        }

        for prop_type, multiplier in type_multipliers.items():
            mask = df['property_type'] == prop_type
            price[mask] *= multiplier

        # Add some noise
        price *= np.random.normal(1, 0.1, n_samples)
        price = np.abs(price)  # Ensure positive prices

        df['price'] = price

        print("Generated synthetic real estate dataset")
        return df

    def explore_data(self, df):
        """Perform comprehensive data exploration"""
        print("=== DATA EXPLORATION ===")

        # Basic info
        print(f"Dataset shape: {df.shape}")
        print(f"Missing values:\n{df.isnull().sum()}")
        print(f"\nData types:\n{df.dtypes}")

        # Target variable analysis
        target_col = 'price' if 'price' in df.columns else df.columns[-1]
        print(f"\n=== TARGET VARIABLE: {target_col.upper()} ===")
        print(f"Mean: ${df[target_col].mean():,.2f}")
        print(f"Median: ${df[target_col].median():,.2f}")
        print(f"Std: ${df[target_col].std():,.2f}")
        print(f"Min: ${df[target_col].min():,.2f}")
        print(f"Max: ${df[target_col].max():,.2f}")

        # Visualizations
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))

        # Price distribution
        axes[0, 0].hist(df[target_col], bins=50, alpha=0.7, color='skyblue')
        axes[0, 0].set_title('Price Distribution')
        axes[0, 0].set_xlabel('Price ($)')
        axes[0, 0].set_ylabel('Frequency')

        # Log price distribution
        log_prices = np.log(df[target_col])
        axes[0, 1].hist(log_prices, bins=50, alpha=0.7, color='lightgreen')
        axes[0, 1].set_title('Log Price Distribution')
        axes[0, 1].set_xlabel('Log Price')
        axes[0, 1].set_ylabel('Frequency')

        # Correlation heatmap (numeric columns only)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        corr_matrix = df[numeric_cols].corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0, 2])
        axes[0, 2].set_title('Feature Correlation Matrix')

        # Price vs key features
        if 'square_feet' in df.columns:
            axes[1, 0].scatter(df['square_feet'], df[target_col], alpha=0.5)
            axes[1, 0].set_xlabel('Square Feet')
            axes[1, 0].set_ylabel('Price ($)')
            axes[1, 0].set_title('Price vs Square Feet')

        if 'bedrooms' in df.columns:
            df.boxplot(column=target_col, by='bedrooms', ax=axes[1, 1])
            axes[1, 1].set_xlabel('Bedrooms')
            axes[1, 1].set_ylabel('Price ($)')
            axes[1, 1].set_title('Price vs Bedrooms')

        # Feature importance (using simple correlation)
        if len(numeric_cols) > 1:
            correlations = df[numeric_cols].corr()[target_col].abs().sort_values(ascending=False)[1:]
            axes[1, 2].barh(range(len(correlations)), correlations.values)
            axes[1, 2].set_yticks(range(len(correlations)))
            axes[1, 2].set_yticklabels(correlations.index)
            axes[1, 2].set_xlabel('Absolute Correlation with Price')
            axes[1, 2].set_title('Feature Importance (Correlation)')

        plt.tight_layout()
        plt.show()

        return df

    def preprocess_data(self, df):
        """Preprocess the data for machine learning"""
        # Identify target column
        target_col = 'price' if 'price' in df.columns else df.columns[-1]

        # Separate features and target
        X = df.drop(columns=[target_col])
        y = df[target_col]

        # Identify numeric and categorical columns
        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

        print(f"Numeric features: {numeric_features}")
        print(f"Categorical features: {categorical_features}")

        # Create preprocessing pipeline
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', StandardScaler(), numeric_features),
                ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)
            ]
        )

        # Store preprocessor
        self.preprocessor = preprocessor

        # Fit and transform the data
        X_processed = preprocessor.fit_transform(X)

        # Get feature names after preprocessing
        numeric_feature_names = numeric_features

        if categorical_features:
            # Get categorical feature names after one-hot encoding
            cat_feature_names = []
            onehot_encoder = preprocessor.named_transformers_['cat']
            for i, feature in enumerate(categorical_features):
                categories = onehot_encoder.categories_[i][1:]  # Skip first (dropped)
                cat_feature_names.extend([f"{feature}_{cat}" for cat in categories])

            self.feature_names = numeric_feature_names + cat_feature_names
        else:
            self.feature_names = numeric_feature_names

        print(f"Processed feature shape: {X_processed.shape}")
        print(f"Feature names: {len(self.feature_names)}")

        return X_processed, y

    def build_neural_network(self, input_dim):
        """Build neural network model for price prediction"""
        model = Sequential([
            Dense(256, activation='relu', input_shape=(input_dim,)),
            BatchNormalization(),
            Dropout(0.3),

            Dense(128, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),

            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.2),

            Dense(32, activation='relu'),
            Dropout(0.2),

            Dense(1, activation='linear')  # Linear output for regression
        ])

        model.compile(
            optimizer='adam',
            loss='mse',
            metrics=['mae']
        )

        return model

    def train_multiple_models(self, X_train, X_test, y_train, y_test):
        """Train and compare multiple models"""
        models = {
            'Neural Network': None,
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
            'Ridge Regression': Ridge(alpha=1.0),
            'Lasso Regression': Lasso(alpha=1.0)
        }

        results = {}

        for name, model in models.items():
            print(f"Training {name}...")

            if name == 'Neural Network':
                # Build and train neural network
                nn_model = self.build_neural_network(X_train.shape[1])

                # Callbacks
                callbacks = [
                    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
                    tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5)
                ]

                # Train
                history = nn_model.fit(
                    X_train, y_train,
                    validation_data=(X_test, y_test),
                    epochs=100,
                    batch_size=32,
                    callbacks=callbacks,
                    verbose=0
                )

                # Predict
                y_pred = nn_model.predict(X_test, verbose=0).flatten()
                model = nn_model

            else:
                # Train traditional ML model
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)

            # Calculate metrics
            mae = mean_absolute_error(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_test, y_pred)

            results[name] = {
                'model': model,
                'mae': mae,
                'mse': mse,
                'rmse': rmse,
                'r2': r2,
                'predictions': y_pred
            }

            print(f"{name} - MAE: ${mae:,.2f}, RMSE: ${rmse:,.2f}, R²: {r2:.4f}")

        return results

    def evaluate_model(self, y_true, y_pred, model_name="Model"):
        """Evaluate model performance with detailed metrics and visualizations"""
        # Calculate metrics
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)

        print(f"=== {model_name.upper()} EVALUATION ===")
        print(f"Mean Absolute Error (MAE): ${mae:,.2f}")
        print(f"Mean Squared Error (MSE): {mse:,.2f}")
        print(f"Root Mean Squared Error (RMSE): ${rmse:,.2f}")
        print(f"R-squared (R²): {r2:.4f}")
        print(f"Mean Absolute Percentage Error: {np.mean(np.abs((y_true - y_pred) / y_true)) * 100:.2f}%")

        # Visualizations
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # Actual vs Predicted
        axes[0, 0].scatter(y_true, y_pred, alpha=0.6, color='blue')
        axes[0, 0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('Actual Price ($)')
        axes[0, 0].set_ylabel('Predicted Price ($)')
        axes[0, 0].set_title(f'Actual vs Predicted Prices\nR² = {r2:.4f}')

        # Residuals
        residuals = y_true - y_pred
        axes[0, 1].scatter(y_pred, residuals, alpha=0.6, color='green')
        axes[0, 1].axhline(y=0, color='r', linestyle='--')
        axes[0, 1].set_xlabel('Predicted Price ($)')
        axes[0, 1].set_ylabel('Residuals ($)')
        axes[0, 1].set_title('Residual Plot')

        # Residual distribution
        axes[1, 0].hist(residuals, bins=30, alpha=0.7, color='orange')
        axes[1, 0].set_xlabel('Residuals ($)')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Residual Distribution')

        # Error distribution
        errors = np.abs(residuals)
        axes[1, 1].hist(errors, bins=30, alpha=0.7, color='purple')
        axes[1, 1].set_xlabel('Absolute Error ($)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title(f'Error Distribution\nMAE = ${mae:,.0f}')

        plt.tight_layout()
        plt.show()

        return {'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2}

    def get_feature_importance(self, model, model_type='rf'):
        """Get and visualize feature importance"""
        if model_type in ['rf', 'gb']:  # Random Forest or Gradient Boosting
            importances = model.feature_importances_

        elif hasattr(model, 'coef_'):  # Linear models
            importances = np.abs(model.coef_)

        else:
            print("Feature importance not available for this model type")
            return None

        # Create importance DataFrame
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)

        # Plot top 20 features
        plt.figure(figsize=(12, 8))
        top_features = importance_df.head(20)
        plt.barh(range(len(top_features)), top_features['importance'].values)
        plt.yticks(range(len(top_features)), top_features['feature'].values)
        plt.xlabel('Feature Importance')
        plt.title('Top 20 Most Important Features')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

        return importance_df

    def predict_single_property(self, property_features):
        """Predict price for a single property"""
        if not self.is_trained:
            print("Model is not trained yet!")
            return None

        # Convert to DataFrame
        if isinstance(property_features, dict):
            property_df = pd.DataFrame([property_features])
        else:
            property_df = property_features

        # Preprocess
        X_processed = self.preprocessor.transform(property_df)

        # Predict
        if hasattr(self.model, 'predict'):
            if 'tensorflow' in str(type(self.model)):
                prediction = self.model.predict(X_processed, verbose=0)[0][0]
            else:
                prediction = self.model.predict(X_processed)[0]
        else:
            prediction = None

        return prediction

    def create_price_estimator_tool(self):
        """Interactive tool for price estimation"""
        print("=== REAL ESTATE PRICE ESTIMATOR ===")

        if not self.is_trained:
            print("Please train a model first!")
            return

        # Sample property for estimation
        sample_property = {
            'bedrooms': 3,
            'bathrooms': 2,
            'square_feet': 2000,
            'lot_size': 8000,
            'year_built': 2010,
            'garage_spaces': 2,
            'floors': 2,
            'location': 'Suburb',
            'property_type': 'Single Family',
            'condition': 4,
            'quality': 4,
            'distance_to_city': 10,
            'distance_to_school': 2,
            'distance_to_shopping': 5,
            'crime_rate': 2,
            'school_rating': 8
        }

        print("Sample property features:")
        for key, value in sample_property.items():
            print(f"  {key}: {value}")

        predicted_price = self.predict_single_property(sample_property)

        if predicted_price:
            print(f"\nPredicted Price: ${predicted_price:,.2f}")

            # Price range analysis
            print("\nPrice Sensitivity Analysis:")

            # Vary key features
            variations = {
                'bedrooms': [2, 3, 4, 5],
                'square_feet': [1500, 2000, 2500, 3000],
                'condition': [2, 3, 4, 5]
            }

            for feature, values in variations.items():
                print(f"\n{feature.upper()} impact:")
                for value in values:
                    temp_property = sample_property.copy()
                    temp_property[feature] = value
                    temp_price = self.predict_single_property(temp_property)
                    price_diff = temp_price - predicted_price
                    print(f"  {feature}={value}: ${temp_price:,.0f} ({price_diff:+,.0f})")

    def save_model(self, model, model_name='real_estate_model'):
        """Save the trained model and preprocessor"""
        # Save model
        if 'tensorflow' in str(type(model)):
            model.save(f'{model_name}.h5')
        else:
            joblib.dump(model, f'{model_name}.pkl')

        # Save preprocessor
        joblib.dump(self.preprocessor, f'{model_name}_preprocessor.pkl')

        # Save feature names
        with open(f'{model_name}_features.txt', 'w') as f:
            for feature in self.feature_names:
                f.write(f"{feature}\n")

        print(f"Model saved as {model_name}")

    def load_model(self, model_path, is_neural_network=False):
        """Load a trained model and preprocessor"""
        try:
            # Load model
            if is_neural_network:
                self.model = tf.keras.models.load_model(model_path)
            else:
                self.model = joblib.load(model_path)

            # Load preprocessor
            preprocessor_path = model_path.replace('.pkl', '_preprocessor.pkl').replace('.h5', '_preprocessor.pkl')
            self.preprocessor = joblib.load(preprocessor_path)

            # Load feature names
            features_path = model_path.replace('.pkl', '_features.txt').replace('.h5', '_features.txt')
            with open(features_path, 'r') as f:
                self.feature_names = [line.strip() for line in f.readlines()]

            self.is_trained = True
            print("Model loaded successfully!")

        except Exception as e:
            print(f"Error loading model: {e}")

def main():
    """Main function demonstrating the real estate price prediction system"""
    # Initialize predictor
    predictor = RealEstatePricePredictor()

    # Load data (using synthetic data for demo)
    print("Loading real estate data...")
    df = predictor.load_data('synthetic')

    # Explore data
    df = predictor.explore_data(df)

    # Preprocess data
    print("\nPreprocessing data...")
    X, y = predictor.preprocess_data(df)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    print(f"Training set: {X_train.shape}")
    print(f"Test set: {X_test.shape}")

    # Train multiple models and compare
    print("\nTraining multiple models...")
    results = predictor.train_multiple_models(X_train, X_test, y_train, y_test)

    # Find best model
    best_model_name = max(results.keys(), key=lambda k: results[k]['r2'])
    best_model = results[best_model_name]['model']
    best_predictions = results[best_model_name]['predictions']

    print(f"\nBest model: {best_model_name}")

    # Detailed evaluation of best model
    predictor.evaluate_model(y_test, best_predictions, best_model_name)

    # Feature importance (if applicable)
    if best_model_name in ['Random Forest', 'Gradient Boosting']:
        model_type = 'rf' if 'Random Forest' in best_model_name else 'gb'
        importance_df = predictor.get_feature_importance(best_model, model_type)

    # Set up for prediction tool
    predictor.model = best_model
    predictor.is_trained = True

    # Interactive price estimation
    predictor.create_price_estimator_tool()

    # Save best model
    predictor.save_model(best_model, f'best_{best_model_name.lower().replace(" ", "_")}_model')

    print("\n=== SUMMARY ===")
    print("Model training completed successfully!")
    print(f"Best model: {best_model_name}")
    print(f"R² Score: {results[best_model_name]['r2']:.4f}")
    print(f"RMSE: ${results[best_model_name]['rmse']:,.2f}")

if __name__ == "__main__":
    main()